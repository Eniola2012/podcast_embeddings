{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the dataset using gpt\n",
    "\n",
    "The purpose of this notebook is to create an easily reproducible method of cleaning and organising the podcaste textfiles using the gpt API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #all functions need to be pre-pended with 'pd.' e.g. the DataFrame function must be written as 'pd.DataFrame'\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import time\n",
    "import os \n",
    "\n",
    "\n",
    "from helper_file import chunk_text_with_overlap, RateLimiter, get_model_response, find_overlap, merge_chunks\n",
    "\n",
    "#from helper_file import * #\n",
    "#Using the star with import means all functions are imported with their normal names.\n",
    "#This is useful as you don't need to pre-pend anything. However, if two libraries have a function of the same \n",
    "#name one of them will be overwritten as python cannot distinguish between them. It is especially risky if you import an entire library instead of \n",
    "#simply specific functions. However, when using helper modules are so specific to the project it usually doesn't matter. \n",
    "#However, you will also import all the functions you have imported in your helper file, this can make it confusing as to what is \n",
    "#actually needed inthe script or notebook, so explicit import is probably the best move\n",
    "\n",
    "\n",
    "# Set up the OpenAI API key from the '.env' file, this allows you to keep your key secret and not expose on github\n",
    "#have the api key in like the below. You need to create a .env file.\n",
    "#OPENAI_API_KEY = \"my api key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in read mode ('r')\n",
    "with open('./data/raw_podcasts/ab_and_jv_oct2023_mapping_sustainability_txt.txt', 'r') as file:\n",
    "    # Read the entire contents of the file into a string\n",
    "    file_contents = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10841\n",
      "10045\n"
     ]
    }
   ],
   "source": [
    "enc = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "\n",
    "tokens0 = enc.encode(file_contents)\n",
    "#remove all new lines and replace with space, replace all double space with single space\n",
    "#This is because the line breaks are basically random and related to time or something in the transcription algo\n",
    "\n",
    "new_string = file_contents.replace(\"\\n\", \" \").replace(\"\\s+\", \" \")\n",
    "tokens = enc.encode(new_string)\n",
    "print(len(tokens0))\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_list = chunk_text_with_overlap(new_string, 1500, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text chunk: 1/7 complete, time taken 34.93 seconds\n",
      "text chunk: 2/7 complete, time taken 17.59 seconds\n",
      "text chunk: 3/7 complete, time taken 33.77 seconds\n",
      "text chunk: 4/7 complete, time taken 28.67 seconds\n",
      "text chunk: 5/7 complete, time taken 31.46 seconds\n",
      "text chunk: 6/7 complete, time taken 34.67 seconds\n",
      "text chunk: 7/7 complete, time taken 26.54 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response_list = []\n",
    "\n",
    "rate_limiter = RateLimiter(50000)\n",
    "\n",
    "chunk_num = 1\n",
    "\n",
    "for chunk in chunk_list:\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    prompt_text = f\"\"\"The below text comes from an automatically transcribed podcast in which an academic interviews \n",
    "    another academic on their research in food systems. \n",
    "    The text has no punctuation and no linebreaks. I need you to add in appropriate punctuation, lines, paragraphs as appropriate, to make the\n",
    "    transcription easier to read. the transcript contains numerous filler words such as 'um', 'uh', 'like', and 'you know.' \n",
    "    Please remove these filler words to make the transcript more concise and easier to read, while ensuring the original meaning and tone of the speaker are preserved. \n",
    "    It's important that the edited transcript remains true to the speaker's intended message and style.\n",
    "    Also note that due to the length of the podcast the test has been broken up into smaller chunks and the text may start or end mid sentence. \n",
    "    The podcast transcription is surrounded by triple colons ':::'\n",
    "    ::: {chunk} :::\n",
    "    \"\"\"\n",
    "\n",
    "    response = get_model_response(prompt_text, 'You are an expert in repairing transcription errors', \n",
    "                                  rate_limiter, engine=\"gpt-3.5-turbo\").choices[0].message.content\n",
    "    \n",
    "    response_list.append(response)\n",
    "\n",
    "    end_time = time.time()  # End timing\n",
    "    chunk_time = end_time - start_time  # Calculate the time taken for this chunk\n",
    "\n",
    "    print(f\"text chunk: {chunk_num}/{len(chunk_list)} complete, time taken {chunk_time:.2f} seconds\")\n",
    "    chunk_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match found maerging text\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "\n",
    "overlap_size = 100  # Assume 10 words overlap\n",
    "\n",
    "prev_chunk = response_list[0]\n",
    "\n",
    "curr_chunk = response_list[1]\n",
    "\n",
    "overlap = find_overlap(prev_chunk, curr_chunk, overlap_size)\n",
    "merged_text = merge_chunks(prev_chunk, curr_chunk, overlap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match found maerging text\n",
      "match found maerging text\n",
      "match found maerging text\n",
      "match found maerging text\n",
      "match found maerging text\n",
      "match found maerging text\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "merged_text = \"\"\n",
    "overlap_size = 100  # Adjust this based on your expected overlap size\n",
    "\n",
    "# Iterate through the chunks\n",
    "for i in range(len(chunk_list)):\n",
    "    # For the first chunk, just set it as the starting point of merged_text\n",
    "    if i == 0:\n",
    "        merged_text = chunk_list[i]\n",
    "    else:\n",
    "        # Find the overlap between the current state of merged_text and the next chunk\n",
    "        overlap = find_overlap(merged_text, chunk_list[i], overlap_size)\n",
    "\n",
    "        # Merge the current state of merged_text with the next chunk using the found overlap\n",
    "        merged_text = merge_chunks(merged_text, chunk_list[i], overlap)\n",
    "\n",
    "# At the end of the loop, merged_text contains all the chunks merged together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The merged text has been saved to ./data/cleaned_podcasts/ab_and_jv_oct2023_mapping_sustainability_txt.txt\n"
     ]
    }
   ],
   "source": [
    "# Assuming merged_text contains your final combined text\n",
    "file_path = os.path.join('./data', 'cleaned_podcasts',\"ab_and_jv_oct2023_mapping_sustainability_txt.txt\" ) # You can change the file path and name as needed\n",
    "\n",
    "# Writing to a file\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(merged_text)\n",
    "\n",
    "print(f\"The merged text has been saved to {file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
